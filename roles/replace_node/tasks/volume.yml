---
# Set up the volume management
- name: Fetch the directory and volume details
  block:
    - name: Get the list of volumes on the machine
      shell: ls "{{ glusterd_libdir }}/vols"
      register: dir_list

    - set_fact:
        volumes: "{{ dir_list.stdout.split() }}"

    # Find the list of bricks on the machine
    - name: Get the list of bricks corresponding to volume
      shell: >
        gluster vol info {{ item }} | grep {{ gluster_maintenance_cluster_node }} |
        awk -F: '{ print $3 }'
      with_items: "{{ volumes }}"
      register: brick_list

    - set_fact:
        volume_bricks: {}

    - name: Get the list of bricks into a variable
      set_fact:
        volume_bricks: "{{ volume_bricks|combine({item.item: item.stdout}) }}"
      loop: "{{ brick_list.results }}"
  delegate_to: "{{ gluster_maintenance_cluster_node }}"
  
- name: Run replace-brick commit on the brick
  shell: >
    gluster volume replace-brick {{ item.key }}
    {{gluster_maintenance_cluster_node}}:{{item.value}}
    {{gluster_maintenance_cluster_node}}:{{item.value}}
    commit force
  loop: "{{ lookup('dict', volume_bricks) }}"
  delegate_to: "{{ gluster_maintenance_cluster_node }}"

- name: Start glusterd on new node
  service:
    name: glusterd
    state: restarted
  delegate_to: "{{ gluster_maintenance_new_node }}"

# Somehow resetting the bricks and restarting glusterd does not start
# the brick processes. Start the brick processes.
# Can't use the gluster_volume module, it does not support foce for
# starting volumes
- name: Run volume start force to bring up brick processes
  shell: >
    gluster volume start {{ item }} force
  with_items: "{{ volumes }}"

