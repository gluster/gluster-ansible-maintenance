---
- name: Get the UUID of the old node
  shell: >
    gluster peer status |
    grep -A1 {{ gluster_maintenance_old_node | mandatory }} |
    awk -F: '/uid/ { print $2}'
  register: old_node_uuid
  run_once: true

- name: Fail if parsed hostname is different from the back-end FQDN
  fail: msg="Hostname mentioned in inventory should be same with back-end gluster FQDN"
  when: old_node_uuid.stdout == ""

# Following tasks are applicable only for replacing with different FQDN
- name: Peer restoration in the case of replacing with different FQDN
  block:
    # probe the new host, if new host is not the same old host
    - name: Peer probe the new host
      gluster_peer:
        state: present
        nodes:
          - "{{ gluster_maintenance_new_node }}"

    # Workaround for Gluster Bug doesn't copy the peer file in rejected state
    - name: Workaround for the bug to copy peer file of reject node
      copy:
        src: "{{ glusterd_libdir }}/peers/{{ old_node_uuid.stdout | trim }}"
        dest: "{{ glusterd_libdir }}/peers/"
      delegate_to: "{{ gluster_maintenance_new_node }}"

    - name: Restart glusterd on the new node
      service:
        name: glusterd
        state: restarted
      delegate_to: "{{ gluster_maintenance_new_node }}"
  when: gluster_maintenance_old_node != gluster_maintenance_new_node

# Following tasks are applicable only for replacing with same FQDN
- name: Peer restoration in the case of replacing with same FQDN
  block:
    - name: Get the UUID of the current node
      shell: >
        cat "{{ glusterd_libdir }}"/glusterd.info |
        awk -F= '/UUID/ { print $2}'
      register: current_node_uuid
      run_once: true

    - name: Fail if current node UUID is empty 
      fail: msg="Execute this playbook on the active host"
      when: current_node_uuid.stdout == ""

    - name: Store the UUID
      set_fact:
        old_uuid: "{{ old_node_uuid.stdout | trim }}"
        current_uuid: "{{ current_node_uuid.stdout | trim }}"

    - name: Collect all the peer files from cluster_node
      copy:
        src: "{{ glusterd_libdir }}/peers/"
        dest: "{{ peer_tmp_dir }}/"
      delegate_to: "{{ gluster_maintenance_cluster_node }}"

    - name: Check whether peer file of cluster_node is available
      stat:
        path: "{{ glusterd_libdir }}/peers/{{ current_uuid }}"
      register: peerfileres
      delegate_to: "{{ gluster_maintenance_cluster_node_2 | mandatory }}" 

    - name: Fetch the peer file of the current node  
      fetch:
        src: "{{ glusterd_libdir }}/peers/{{ current_uuid }}"
        dest: "{{ peer_tmp_dir }}/"
        flat: yes
      delegate_to: "{{ gluster_maintenance_cluster_node_2 | mandatory }}"
      when: peerfileres.stat.isreg is defined

    - name: Fetch the peer file of the current node  
      fetch:
        src: "{{ glusterd_libdir }}/peers/{{ current_uuid }}"
        dest: "{{ peer_tmp_dir }}/"
        flat: yes
      delegate_to: "{{ gluster_maintenance_cluster_node | mandatory }}"
      when: peerfileres.stat.isreg is not defined

    - name: Remove the old node uuid from the extracted peer details
      file:
        path: "{{ peer_tmp_dir }}/{{ old_uuid }}"
        state: absent
      delegate_to: "{{ gluster_maintenance_cluster_node }}"

    - name: Copy all the peer files to the new host
      copy:
        src: "{{ peer_tmp_dir }}/"
        dest: "{{ glusterd_libdir }}/peers/"
      delegate_to: "{{ gluster_maintenance_new_node }}"
  
    - name: Edit the new node's glusterd.info
      lineinfile:
        path: "{{ glusterd_libdir }}/glusterd.info"
        regexp: '^UUID='
        line: "UUID={{ old_uuid }}"
      delegate_to: "{{ gluster_maintenance_new_node | mandatory }}"

    - name: Restart glusterd
      service:
        name: glusterd
        state: restarted
      delegate_to: "{{ gluster_maintenance_new_node | mandatory }}"

    - pause: seconds=5
  when: gluster_maintenance_old_node == gluster_maintenance_new_node
